{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3qiUCPDDBoe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import dataloader\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "device='cuda' if torch.cuda.is_available() else 'cpu'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7reL0N3XXEF"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfy6W3QoYTni"
      },
      "source": [
        "Hyparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PY35UKK5YKOM"
      },
      "outputs": [],
      "source": [
        "Batch_size=512\n",
        "Epochs=100\n",
        "Learning_rate=3e-4\n",
        "Patch_size=4\n",
        "Num_classes=10\n",
        "img_size=32\n",
        "Channels=3\n",
        "Embed_dim=384\n",
        "Num_heads=8\n",
        "Depth=6\n",
        "Mlp_dim=512\n",
        "Drop_rate=0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdQ3ezBQ6lPi"
      },
      "source": [
        "Image augmentation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpNKfpbs6k9O"
      },
      "outputs": [],
      "source": [
        "mean = (0.4914, 0.4822, 0.4465)\n",
        "std = (0.2023, 0.1994, 0.2010)\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.Resize(img_size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(img_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRgs2QNpaKmt"
      },
      "source": [
        "Dataset[CIFAR-10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAoC-DABaGly"
      },
      "outputs": [],
      "source": [
        "train_dataset=datasets.CIFAR10(root='./data',train=True,download=True,transform=transform_train)\n",
        "test_dataset=datasets.CIFAR10(root='./data',train=False,download=True,transform=transform_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADeg0QkebBEb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1476d144-4bad-44ae-d22a-ceeacfc4ec41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "train_dataloader  =torch.utils.data.DataLoader(train_dataset,batch_size=Batch_size,num_workers=8,shuffle=True)\n",
        "test_dataloader =torch.utils.data.DataLoader(test_dataset,batch_size=Batch_size,num_workers=8, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGA79y3wcJl9"
      },
      "source": [
        "ViT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bGVRiUccPD8"
      },
      "outputs": [],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self,img_size,patch_size,in_channels,embed_dim):\n",
        "        super().__init__()\n",
        "        self.patch_size=patch_size\n",
        "\n",
        "        ## for non overlapping patches:\n",
        "        self.proj=nn.Conv2d(in_channels,embed_dim,kernel_size=patch_size,stride=patch_size)\n",
        "        num_patches=(img_size//patch_size)**2\n",
        "\n",
        "        ## overlapping patches:\n",
        "        #self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size // 2)\n",
        "        #output_height = (img_size - patch_size) // (patch_size // 2) + 1\n",
        "        #output_width = (img_size - patch_size) // (patch_size // 2) + 1\n",
        "        #num_patches = output_height * output_width\n",
        "        ##\n",
        "        self.cls_token=nn.Parameter(torch.randn(1,1,embed_dim))\n",
        "        self.pos_embed=nn.Parameter(torch.randn(1,num_patches+1,embed_dim))\n",
        "    def forward(self,x):\n",
        "        B=x.size(0)\n",
        "        x=self.proj(x)\n",
        "        x=x.flatten(2).transpose(1,2)\n",
        "        cls_token=self.cls_token.expand(B,-1,-1)\n",
        "        x=torch.cat((cls_token,x),dim=1)\n",
        "        x=x+self.pos_embed\n",
        "        return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self,in_features,hidden_features,drop_rate):\n",
        "        super().__init__()\n",
        "        self.fc1=nn.Linear(in_features,hidden_features)\n",
        "        self.fc2=nn.Linear(hidden_features,in_features)\n",
        "        self.drop=nn.Dropout(drop_rate)\n",
        "    def forward(self,x):\n",
        "        x=self.drop(F.gelu(self.fc1(x)))\n",
        "        x=self.drop(self.fc2(x))\n",
        "        return x\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self,embed_dim,num_heads,mlp_dim,drop_rate):\n",
        "        super().__init__()\n",
        "        self.norm1=nn.LayerNorm(embed_dim)\n",
        "        self.attn=nn.MultiheadAttention(embed_dim,num_heads,dropout=drop_rate, batch_first= True)\n",
        "        self.norm2=nn.LayerNorm(embed_dim)\n",
        "        self.mlp=MLP(embed_dim,mlp_dim,drop_rate)\n",
        "    def forward(self,x):\n",
        "        x=x+self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
        "        x=x+self.mlp(self.norm2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIDOZBdMcI5I"
      },
      "outputs": [],
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "  def __init__(self, img_size, patch_size, in_channels, num_classes, embed_dim, depth, num_heads, mlp_dim,drop_rate):\n",
        "      super().__init__()\n",
        "      self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
        "      self.encoder =nn.Sequential(*[TransformerEncoderBlock(embed_dim,num_heads,mlp_dim,drop_rate)\n",
        "       for _ in range(depth)])\n",
        "      self.norm = nn.LayerNorm(embed_dim)\n",
        "      self.head = nn.Linear(embed_dim, num_classes)\n",
        "  def forward(self, x):\n",
        "     x = self.patch_embed(x)\n",
        "     x = self.encoder(x)\n",
        "     x = self.norm(x)\n",
        "     cls_token = x[:,0]\n",
        "     return self.head(cls_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnrYr2Hajpj8"
      },
      "source": [
        "Initiation of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCri0BxLjmE9",
        "outputId": "bad6e3ae-aa1c-4ef3-f1a1-682e38f7a234"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (patch_embed): PatchEmbedding(\n",
              "    (proj): Conv2d(3, 384, kernel_size=(4, 4), stride=(4, 4))\n",
              "  )\n",
              "  (encoder): Sequential(\n",
              "    (0): TransformerEncoderBlock(\n",
              "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "      )\n",
              "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=384, out_features=512, bias=True)\n",
              "        (fc2): Linear(in_features=512, out_features=384, bias=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (1): TransformerEncoderBlock(\n",
              "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "      )\n",
              "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=384, out_features=512, bias=True)\n",
              "        (fc2): Linear(in_features=512, out_features=384, bias=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (2): TransformerEncoderBlock(\n",
              "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "      )\n",
              "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=384, out_features=512, bias=True)\n",
              "        (fc2): Linear(in_features=512, out_features=384, bias=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (3): TransformerEncoderBlock(\n",
              "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "      )\n",
              "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=384, out_features=512, bias=True)\n",
              "        (fc2): Linear(in_features=512, out_features=384, bias=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (4): TransformerEncoderBlock(\n",
              "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "      )\n",
              "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=384, out_features=512, bias=True)\n",
              "        (fc2): Linear(in_features=512, out_features=384, bias=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (5): TransformerEncoderBlock(\n",
              "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
              "      )\n",
              "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=384, out_features=512, bias=True)\n",
              "        (fc2): Linear(in_features=512, out_features=384, bias=True)\n",
              "        (drop): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "  (head): Linear(in_features=384, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "model = VisionTransformer(img_size, Patch_size, Channels, Num_classes, Embed_dim, Depth, Num_heads, Mlp_dim, Drop_rate).to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQM5GhnVmD34"
      },
      "source": [
        "LOSS and Optimiser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNHPSRG8l8TY"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=Epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HBjPYLzmvVZ"
      },
      "outputs": [],
      "source": [
        "def train(model,dataloader,criterion,optimizer,device):\n",
        "    model.train()\n",
        "    total_loss, correct =0 ,0\n",
        "    for x,y in dataloader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(x)\n",
        "        loss = criterion(output,y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()*x.size(0)\n",
        "        correct += (output.argmax(1)==y).sum().item()\n",
        "    return total_loss/len(dataloader.dataset), correct/len(dataloader.dataset)\n",
        "def evaluate(model,dataloader,criterion,device):\n",
        "    model.eval()\n",
        "    total_loss, correct =0 ,0\n",
        "    with torch.inference_mode():\n",
        "      for x,y in dataloader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        output = model(x)\n",
        "        correct += (output.argmax(dim=1)==y).sum().item()\n",
        "    return correct/len(dataloader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxvO_cf9pA-A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 934,
          "referenced_widgets": [
            "acb7c6c139c74818ac7bdfde5f6ab13c",
            "604581257355473ea1ad94b8059ef6e6",
            "62332f4bb7c74dddadf669fc397bcdd8",
            "d25aace5948c4f5ead4880fab615f6b4",
            "53562e7232d14dcabbc0142288a57020",
            "5da88c99bc444279b88e4c3c1e0cbefc",
            "1cb389a9fd8f4c59bc93b6d9dbd9d139",
            "e43f234936e64e178775e3bb698d29c7",
            "263869cdd0dd4e8fa74c5ea5bd064038",
            "66aef31ed20d41f798331cedbd741639",
            "c59720b96736483c934dcbf3fa13d3e1"
          ]
        },
        "outputId": "559ab775-6a41-43f5-c15d-9a4e6e19989e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "acb7c6c139c74818ac7bdfde5f6ab13c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 1/100,Train Loss:1.9572, Train acc: 0.3166, Test acc: 0.4407\n",
            "Epochs: 2/100,Train Loss:1.6888, Train acc: 0.4533, Test acc: 0.5078\n",
            "Epochs: 3/100,Train Loss:1.5946, Train acc: 0.4977, Test acc: 0.5404\n",
            "Epochs: 4/100,Train Loss:1.5215, Train acc: 0.5313, Test acc: 0.5658\n",
            "Epochs: 5/100,Train Loss:1.4597, Train acc: 0.5605, Test acc: 0.6011\n",
            "Epochs: 6/100,Train Loss:1.4145, Train acc: 0.5838, Test acc: 0.6103\n",
            "Epochs: 7/100,Train Loss:1.3747, Train acc: 0.6016, Test acc: 0.6214\n",
            "Epochs: 8/100,Train Loss:1.3438, Train acc: 0.6164, Test acc: 0.6414\n",
            "Epochs: 9/100,Train Loss:1.3091, Train acc: 0.6315, Test acc: 0.6564\n",
            "Epochs: 10/100,Train Loss:1.2870, Train acc: 0.6425, Test acc: 0.6702\n",
            "Epochs: 11/100,Train Loss:1.2612, Train acc: 0.6520, Test acc: 0.6757\n",
            "Epochs: 12/100,Train Loss:1.2318, Train acc: 0.6676, Test acc: 0.6904\n",
            "Epochs: 13/100,Train Loss:1.2154, Train acc: 0.6764, Test acc: 0.6919\n",
            "Epochs: 14/100,Train Loss:1.1932, Train acc: 0.6868, Test acc: 0.6995\n",
            "Epochs: 15/100,Train Loss:1.1752, Train acc: 0.6942, Test acc: 0.7059\n",
            "Epochs: 16/100,Train Loss:1.1558, Train acc: 0.7039, Test acc: 0.6965\n",
            "Epochs: 17/100,Train Loss:1.1374, Train acc: 0.7116, Test acc: 0.7124\n",
            "Epochs: 18/100,Train Loss:1.1164, Train acc: 0.7216, Test acc: 0.7182\n",
            "Epochs: 19/100,Train Loss:1.1068, Train acc: 0.7247, Test acc: 0.7322\n",
            "Epochs: 20/100,Train Loss:1.0890, Train acc: 0.7356, Test acc: 0.7390\n",
            "Epochs: 21/100,Train Loss:1.0782, Train acc: 0.7390, Test acc: 0.7345\n",
            "Epochs: 22/100,Train Loss:1.0603, Train acc: 0.7460, Test acc: 0.7358\n",
            "Epochs: 23/100,Train Loss:1.0464, Train acc: 0.7547, Test acc: 0.7393\n",
            "Epochs: 24/100,Train Loss:1.0314, Train acc: 0.7593, Test acc: 0.7517\n",
            "Epochs: 25/100,Train Loss:1.0217, Train acc: 0.7653, Test acc: 0.7557\n",
            "Epochs: 26/100,Train Loss:1.0078, Train acc: 0.7722, Test acc: 0.7532\n",
            "Epochs: 27/100,Train Loss:0.9973, Train acc: 0.7772, Test acc: 0.7544\n",
            "Epochs: 28/100,Train Loss:0.9852, Train acc: 0.7817, Test acc: 0.7597\n",
            "Epochs: 29/100,Train Loss:0.9699, Train acc: 0.7899, Test acc: 0.7564\n",
            "Epochs: 30/100,Train Loss:0.9632, Train acc: 0.7918, Test acc: 0.7709\n",
            "Epochs: 31/100,Train Loss:0.9509, Train acc: 0.7983, Test acc: 0.7760\n",
            "Epochs: 32/100,Train Loss:0.9336, Train acc: 0.8064, Test acc: 0.7698\n",
            "Epochs: 33/100,Train Loss:0.9236, Train acc: 0.8116, Test acc: 0.7689\n",
            "Epochs: 34/100,Train Loss:0.9118, Train acc: 0.8165, Test acc: 0.7772\n",
            "Epochs: 35/100,Train Loss:0.9019, Train acc: 0.8211, Test acc: 0.7799\n",
            "Epochs: 36/100,Train Loss:0.8920, Train acc: 0.8256, Test acc: 0.7847\n",
            "Epochs: 37/100,Train Loss:0.8803, Train acc: 0.8316, Test acc: 0.7836\n",
            "Epochs: 38/100,Train Loss:0.8676, Train acc: 0.8375, Test acc: 0.7849\n",
            "Epochs: 39/100,Train Loss:0.8587, Train acc: 0.8403, Test acc: 0.7829\n",
            "Epochs: 40/100,Train Loss:0.8506, Train acc: 0.8437, Test acc: 0.7860\n",
            "Epochs: 41/100,Train Loss:0.8360, Train acc: 0.8522, Test acc: 0.7843\n",
            "Epochs: 42/100,Train Loss:0.8281, Train acc: 0.8560, Test acc: 0.7907\n",
            "Epochs: 43/100,Train Loss:0.8155, Train acc: 0.8603, Test acc: 0.7952\n",
            "Epochs: 44/100,Train Loss:0.8117, Train acc: 0.8628, Test acc: 0.7992\n",
            "Epochs: 45/100,Train Loss:0.7956, Train acc: 0.8718, Test acc: 0.7927\n",
            "Epochs: 46/100,Train Loss:0.7923, Train acc: 0.8707, Test acc: 0.7929\n",
            "Epochs: 47/100,Train Loss:0.7828, Train acc: 0.8768, Test acc: 0.7943\n",
            "Epochs: 48/100,Train Loss:0.7721, Train acc: 0.8821, Test acc: 0.7969\n",
            "Epochs: 49/100,Train Loss:0.7681, Train acc: 0.8834, Test acc: 0.7971\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "best_acc = 0.0\n",
        "patience = 5\n",
        "counter = 0\n",
        "train_accuracies, test_accuracies = [], []\n",
        "for epoch in tqdm(range(Epochs)):\n",
        "    train_loss, train_acc = train(model, train_dataloader, criterion, optimizer, device)\n",
        "    test_acc = evaluate(model, test_dataloader, criterion, device)\n",
        "    train_accuracies.append(train_acc)\n",
        "    test_accuracies.append(test_acc)\n",
        "    print(f\"Epochs: {epoch+1}/{Epochs},Train Loss:{train_loss:.4f}, Train acc: {train_acc:.4f}, Test acc: {test_acc:.4f}\")\n",
        "    scheduler.step()\n",
        "    if test_acc > best_acc:\n",
        "      best_acc = test_acc\n",
        "      counter = 0\n",
        "      torch.save(model.state_dict(), \"best_model.pth\")\n",
        "    else:\n",
        "      counter += 1\n",
        "      if counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "model.load_state_dict(torch.load(\"best_model.pth\"))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}